{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_and_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)\n",
    "\n",
    "def add_double_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text (and double it, called 'weighting up')\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. ' + x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letters_only(text, field):\n",
    "    \"\"\"Replace all non-alphanumeric characters with a space\"\"\"\n",
    "    try:\n",
    "        x = re.sub(\"[^a-zA-Z]\",       # The pattern to search for\n",
    "                   \" \",               # The pattern to replace it with\n",
    "                   text[field] )      # The text to search\n",
    "    except:\n",
    "        return ('byte_code_error_ignore_this_ record')\n",
    "    return (x.lower())\n",
    "\n",
    "def remove_stop_words(text, field, stopwords_set):\n",
    "    \"\"\"Remove stop words from the review text\"\"\"\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"This is a helper function to translate part of speech (POS) for us in the make_lemmas function.\n",
    "    nltk uses pos_tag to determine the POS of a word that is not compatible with the wordnet_lemmatizer.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def make_lemmas(text, field):#, stopwords_set):\n",
    "    \"\"\"Toeknizes all words in the review and then tags them with the part of speech (POS) they belong to\n",
    "    as a tuple. Each tuple (word, pos) is then lemmatized before stop words are removed and the list is\n",
    "    joined back into a single item/doc\"\"\"\n",
    "    x = word_tokenize(text[field])\n",
    "    #x = word_tokenize(x)\n",
    "    x = nltk.pos_tag(x)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    doc = []\n",
    "    for word, part in x:\n",
    "        #doc.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "        doc.append(wnl.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "    #words = [w for w in doc if not w.lower() in stopwords_set]\n",
    "    #x = ( \" \".join( words ))\n",
    "    x = ( \" \".join( doc ))\n",
    "    return(x)\n",
    "\n",
    "def create_neg_stops():\n",
    "    \"\"\"Combine the original list of stop words the negative suffix. For example: 'his', 'they', and 'me' become\n",
    "    'his_neg', 'they_neg', and 'me_neg'.\"\"\"\n",
    "    orig_stops = stopwords.words(\"english\")\n",
    "    neg_stops = []\n",
    "    for i in orig_stops:\n",
    "        neg_stops.append(i+'_neg')\n",
    "    orig_stops.extend(neg_stops)\n",
    "    return(orig_stops)\n",
    "\n",
    "def remove_negated_stop_words(text, field, neg_stops):\n",
    "    \"\"\"Remove all instances of stop words that have the '_neg' suffix.\"\"\"\n",
    "    # Make the text lowercase\n",
    "    x = text[field]\n",
    "    x = x.lower()\n",
    "    \n",
    "    stopwords_set = set(neg_stops)\n",
    "    \n",
    "    # List comprehension that splits the review into words and removes negative stop words\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "#/////////////////////ACCOUNT FOR NEGATION SENTIMENT///////////////////////////////////\n",
    "def negatize(text, field):\n",
    "    \"\"\"Use the NLTK library's mark_negation to find negative words (like 'not' and 'nor') and append\n",
    "    the '_neg' suffix to all words following the first negative word until it encounters a period or comma.\n",
    "    Example: 'I don't like eating pizza, I love eating pizza' \n",
    "    becomes 'I don't like_neg eating_neg pizza_neg, I love pizza' \"\"\"\n",
    "    x = text[field]\n",
    "    \n",
    "    # The TextBlob class provides an easy way to split the reviews into sentences\n",
    "    x = TextBlob(x)\n",
    "    \n",
    "    piece = []\n",
    "    for sentence in x.sentences:\n",
    "        \n",
    "        # Split sentence on commas to ID phrases that need to be negated (if required)\n",
    "        part = re.split(', ',str(sentence))\n",
    "        for i in part:\n",
    "            piece.append(mark_negation(i.split()))\n",
    "    \n",
    "    # Combine all terms/phrases back to one doc\n",
    "    total = []\n",
    "    for terms in piece:\n",
    "        total.append(\" \".join(terms))\n",
    "    review = ''\n",
    "    for phrase in total:\n",
    "        review += phrase + ' '\n",
    "    \n",
    "    # mark_negation adds the _NEG suffix after the period, this catches those and fixes it\n",
    "    review = review.replace(\"._NEG\",\"_NEG.\")\n",
    "    review = review.lower()\n",
    "    \n",
    "    # return the entire entire review except for the last character which is always a space\n",
    "    return (review[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_scent(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    scents = ['scent', 'scents', 'scented', 'smell', 'smells', 'smelled', 'odor', 'odors', 'fragrance', 'fragrances',\n",
    "              'fragrant', 'aroma', 'aromas', 'perfume', 'perfumes', 'perfumed', 'whiff', 'fresh', 'freshness',\n",
    "              'stench', 'stink', 'stinks', 'smelly', 'pungent', 'stinky', 'odoriferous']\n",
    "    for word in x:\n",
    "        if word.lower() in scents:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_moisture(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    moisture = ['moisture', 'moist', 'wet', 'dry', 'damp', 'dampish', 'wettish',  'drenched', 'dripping',\n",
    "                'saturate', 'saturated', 'soaked', 'soaking', 'sodden', 'soggy', 'sopping', 'soppy',\n",
    "                'arid', 'dryness', 'dried', 'waterless', 'bone-dry', 'dehydrated', 'ultradry', 'dank', 'supple',\n",
    "                'quick-drying']\n",
    "    for word in x:\n",
    "        if word.lower() in moisture:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_residue(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    residue = ['streak', 'streaks', 'streaked', 'streaking', 'residue', 'film', 'cloudy', 'drop', 'drip', 'lines'\n",
    "               'drips', 'dripped', 'gunk', 'tarnish', 'spot', 'spots', 'drip-streaks']\n",
    "    for word in x:\n",
    "        if word.lower() in residue:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_value(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    value = ['value', 'price', 'prices', 'worth', 'worthless', 'cost', 'costs', 'expense', 'expensive', 'costly',\n",
    "             'overpriced', 'pricey', 'valuable', 'invaluable', 'cheap', 'economical', 'reasonable', 'inexpensive',\n",
    "             'expenses', 'valued', 'priced', 'cheapen', 'economics', 'low-priced', 'bargain', 'low-cost', 'budget',\n",
    "             'cheapest', 'bargains', 'money', 'budgeted', 'budgets']\n",
    "    for word in x:\n",
    "        if word.lower() in value:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_sensitivity(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    sensitivity = ['sensitivity', 'sensitive', 'skin', 'soft', 'softest', 'soften', 'rough', 'rougher', 'smooth', 'touch']\n",
    "    for word in x:\n",
    "        if word.lower() in sensitivity:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_topics(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    tags = {}\n",
    "    scent_tag = []\n",
    "    moisture_tag = []\n",
    "    residue_tag = []\n",
    "    value_tag = []\n",
    "    sensitivity_tag = []\n",
    "    scents = ['scent', 'scents', 'scented', 'smell', 'smells', 'smelled', 'odor', 'odors', 'fragrance', 'fragrances',\n",
    "              'fragrant', 'aroma', 'aromas', 'perfume', 'perfumes', 'perfumed', 'whiff', 'fresh', 'freshness',\n",
    "              'stench', 'stink', 'stinks', 'smelly', 'pungent', 'stinky', 'odoriferous']\n",
    "    moisture = ['moisture', 'moist', 'wet', 'dry', 'damp', 'dampish', 'wettish',  'drenched', 'dripping',\n",
    "                'saturate', 'saturated', 'soaked', 'soaking', 'sodden', 'soggy', 'sopping', 'soppy',\n",
    "                'arid', 'dryness', 'dried', 'waterless', 'bone-dry', 'dehydrated', 'ultradry', 'dank', 'supple',\n",
    "                'quick-drying']   \n",
    "    residue = ['streak', 'streaks', 'streaked', 'streaking', 'residue', 'film', 'cloudy', 'drop', 'drip', 'lines'\n",
    "               'drips', 'dripped', 'gunk', 'tarnish', 'spot', 'spots', 'drip-streaks']\n",
    "    value = ['value', 'price', 'prices', 'worth', 'worthless', 'cost', 'costs', 'expense', 'expensive', 'costly',\n",
    "             'overpriced', 'pricey', 'valuable', 'invaluable', 'cheap', 'economical', 'reasonable', 'inexpensive',\n",
    "             'expenses', 'valued', 'priced', 'cheapen', 'economics', 'low-priced', 'bargain', 'low-cost', 'budget',\n",
    "             'cheapest', 'bargains', 'money', 'budgeted', 'budgets', 'discount']\n",
    "    sensitivity = ['sensitivity', 'sensitive', 'skin', 'soft', 'softest', 'soften', 'rough', 'smooth', 'touch']\n",
    "    \n",
    "    for word in x:\n",
    "        if word.lower() in scents:\n",
    "            scent_tag.append(word.lower())\n",
    "        if word.lower() in moisture:\n",
    "            moisture_tag.append(word.lower())\n",
    "        if word.lower() in residue:\n",
    "            residue_tag.append(word.lower())\n",
    "        if word.lower() in value:\n",
    "            value_tag.append(word.lower())\n",
    "        if word.lower() in sensitivity:\n",
    "            sensitivity_tag.append(word.lower())\n",
    "    if len(scent_tag) > 0:\n",
    "        tags['scent'] = scent_tag\n",
    "    if len(moisture_tag) > 0:\n",
    "        tags['moisture'] = moisture_tag\n",
    "    if len(residue_tag) > 0:\n",
    "        tags['residue'] = residue_tag\n",
    "    if len(value_tag) > 0:\n",
    "        tags['value'] = value_tag\n",
    "    if len(sensitivity_tag) > 0:\n",
    "        tags['sensitivity'] = sensitivity_tag\n",
    "        \n",
    "    if len(tags) == 0:\n",
    "        return ('')\n",
    "    return (tags)\n",
    "\n",
    "def get_topics(text):\n",
    "    topics = []\n",
    "    if (text['scent']) == 'Yes':\n",
    "        topics.append('scent')\n",
    "    if (text['moisture']) == 'Yes':\n",
    "        topics.append('moisture')\n",
    "    if (text['residue']) == 'Yes':\n",
    "        topics.append('residue')\n",
    "    if (text['value']) == 'Yes':\n",
    "        topics.append('value')\n",
    "    if (text['sensitivity']) == 'Yes':\n",
    "        topics.append('sensitivity')    \n",
    "    return( \", \".join( topics ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nouns_and_adjectives(text,field,stopwords_set):\n",
    "    \"\"\"Starts with the data that alread has had the stops removed. Tokenizes the text and uses nltk to\n",
    "    tag the parts of speech. If the word has a part of speech that is not a noun or adjective, it is not \n",
    "    included in the result.\"\"\"\n",
    "    x = word_tokenize(text[field])\n",
    "    #x = word_tokenize(x)\n",
    "    x = nltk.pos_tag(x)\n",
    "    doc = []\n",
    "    for word, part in x:\n",
    "        if part.startswith('J'):\n",
    "            # This is an adjective\n",
    "            doc.append(word)\n",
    "        if part.startswith('N'):\n",
    "            # This is a noun\n",
    "            doc.append(word)\n",
    "        #doc.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "    words = [w for w in doc if not w.lower() in stopwords_set]\n",
    "    x = ( \" \".join( words ))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data...\n",
      "Finished In:     0.913s.\n",
      "Adding titles to text...\n",
      "Finished In:     0.710s.\n",
      "Removing Non-Alphanumerics...\n",
      "Finished In:     2.403s.\n",
      "Removing Stop Words...\n",
      "Finished In:     1.808s.\n",
      "Lemmatizing The Reviews...\n",
      "Finished In:     399.572s.\n",
      "Lemmatizing The Sentences...\n",
      "Finished In:     514.582s.\n",
      "Use only Nouns and Adjectives...\n",
      "Finished In:     662.649s.\n",
      "Tagging topics for each review...\n",
      "Finished In:     35.059s.\n",
      "Tagging topics for each sentence...\n",
      "Finished In:     40.343s.\n",
      "FINISHED: \n",
      "Time Elapsed:    1659.530s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "t1 = time()\n",
    "#/////////////////////READ THE DATA////////////////////////////////////////////////////\n",
    "print ('Reading the data...')\n",
    "wipes = pd.read_csv(\"home_products.csv\", header=0, encoding=\"ISO-8859-1\" )\n",
    "sentences = pd.read_csv(\"only_sentences.csv\", header=0, encoding=\"ISO-8859-1\" )\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////ADD THE TITLE TO THE TEXT////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Adding titles to text...')\n",
    "wipes['text_and_title'] = wipes.apply(lambda text: text_and_title(text), axis=1)\n",
    "wipes['double_title'] = wipes.apply(lambda text: add_double_title(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NON-ALPHANUMERICS AND PUNCTUATION/////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Non-Alphanumerics...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'text_and_title'), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'double_title'), axis=1)\n",
    "sentences['lowercase_no_punctuation'] = sentences.apply(lambda text: letters_only(text, 'Sentence'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LOAD THE STOPWORDS PROVIDED BY NLTK//////////////////////////////\n",
    "stop1 = ['wipes', 'wipe', 'love', 'like', 'good', 'nice', 'amazon', 'loved', 'loves', 'likes', 'liked']\n",
    "stops = stopwords.words(\"english\")\n",
    "stops.extend(stop1)\n",
    "stopwords_set = set(stops)\n",
    "\n",
    "#/////////////////////REMOVE STOP WORDS///////////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Stop Words...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'text_and_title_no_stops', stopwords_set), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'double_title_no_stops', stopwords_set), axis=1)\n",
    "sentences['lowercase_no_stops'] = sentences.apply(lambda text: remove_stop_words(text, 'lowercase_no_punctuation', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////NEGATE TEXT AND TITLES///////////////////////////////////////////\n",
    "#t1 = time()\n",
    "#print ('Tagging Negative Text...')\n",
    "#wipes['text_and_title_negation'] = wipes.apply(lambda text: negatize(text, 'text_and_title'), axis=1)\n",
    "#wipes['double_title_negation'] = wipes.apply(lambda text: negatize(text, 'double_title'), axis=1)\n",
    "#print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NEGATIVE STOPS////////////////////////////////////////////\n",
    "#t1 = time()\n",
    "#print ('Removing Negative Stop Words...')\n",
    "#wipes['text_and_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'text_and_title_negation', create_neg_stops()), axis=1)\n",
    "#wipes['double_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'double_title_negation', create_neg_stops()), axis=1)\n",
    "#print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LEMMATIZE THE TEXT REVIEWS///////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Lemmatizing The Reviews...')\n",
    "wipes['lemma_text_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'text_and_title_no_stops'), axis=1)\n",
    "wipes['lemma_double_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'double_title_no_stops'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "t1 = time()\n",
    "print ('Lemmatizing The Sentences...')\n",
    "sentences['lemmatized'] = sentences.apply(lambda text: make_lemmas(text, 'lowercase_no_stops'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////NOUNS AND AJECTIVES ONLY/////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Use only Nouns and Adjectives...')\n",
    "wipes['nouns_and_adjectives'] = wipes.apply(lambda text: nouns_and_adjectives(text, 'Text', stopwords_set), axis=1)\n",
    "sentences['nouns_and_adjectives'] = sentences.apply(lambda text: nouns_and_adjectives(text, 'Sentence', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////TAG TOPICS///////////////////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Tagging topics for each review...')\n",
    "wipes['scent'] = wipes.apply(lambda text: tag_scent(text, 'Text'), axis=1)\n",
    "wipes['moisture'] = wipes.apply(lambda text: tag_moisture(text, 'Text'), axis=1)\n",
    "wipes['residue'] = wipes.apply(lambda text: tag_residue(text, 'Text'), axis=1)\n",
    "wipes['value'] = wipes.apply(lambda text: tag_value(text, 'Text'), axis=1)\n",
    "wipes['sensitivity'] = wipes.apply(lambda text: tag_sensitivity(text, 'Text'), axis=1)\n",
    "wipes['tags'] = wipes.apply(lambda text: tag_topics(text, 'Text'), axis=1)\n",
    "wipes['topics'] = wipes.apply(lambda text: get_topics(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "t1 = time()\n",
    "print ('Tagging topics for each sentence...')\n",
    "sentences['scent'] = sentences.apply(lambda text: tag_scent(text, 'Sentence'), axis=1)\n",
    "sentences['moisture'] = sentences.apply(lambda text: tag_moisture(text, 'Sentence'), axis=1)\n",
    "sentences['residue'] = sentences.apply(lambda text: tag_residue(text, 'Sentence'), axis=1)\n",
    "sentences['value'] = sentences.apply(lambda text: tag_value(text, 'Sentence'), axis=1)\n",
    "sentences['sensitivity'] = sentences.apply(lambda text: tag_sensitivity(text, 'Sentence'), axis=1)\n",
    "sentences['tags'] = sentences.apply(lambda text: tag_topics(text, 'Sentence'), axis=1)\n",
    "sentences['topics'] = sentences.apply(lambda text: get_topics(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "\n",
    "#/////////////////////WRITE THE DATA///////////////////////////////////////////////////\n",
    "wipes.to_csv('home_products_additional_features.csv', index=False)\n",
    "sentences.to_csv('sentences_additional_features.csv', index=False)\n",
    "\n",
    "print(\"FINISHED: \\nTime Elapsed:    %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_list = [\"ACT EOS\",\"ACT EOS 2014-2015\",\"ACT Score Data File\",\"AP Score Data File\",\n",
    "               \"AWCOMAD Portfolio Review\",\"Banner GR Recruit Migration\",\"Banner Online Inquiry Form\",\n",
    "               \"Banner Recruit Migration\",\"Cappex\",\"Carnegie AC&U\",\"Carnegie AC&U-CSV\",\n",
    "               \"Carnegie College Xpress\",\"Carnegie PC&U\",\"CBSS\",\"CBSS RY 2015\",\"Chegg\",\n",
    "               \"Chegg Global - Freshman\",\"Chegg Global - Transfer\",\"Chegg Int'l - Freshman\",\n",
    "               \"Chegg Int'l- Transfer\",\"Common App\",\"Common App Material Import\",\n",
    "               \"Common App Prospects\",\"Common App Recommendations\",\"Common App Suspects\",\n",
    "               \"Common App Writing\",\"Delete-don't use\",\"Generic Recruit File\",\"GMAT Score Data File\",\n",
    "               \"GR Big XII Resume Book prospect load\",\"GR CUR Data\",\"GR GRE Name Buys\",\"GR Petersons Leads\",\n",
    "               \"GR School of Hospitality Inquiries\",\"GR SPH Web Inquiries\",\"GR STATCO Inquiry Card\",\n",
    "               \"GR TIPH Registrants prospect load\",\"GRE Score Data File\",\n",
    "               \"High School Scholars - Banner Recruit Migration\",\"IELTS Score Data File\",\n",
    "               \"MCAT Name/Score Buy\",\"MyMajors\",\"NACAC\",\"NDN\",\"Non Responder Import\",\"NRCCUA\",\n",
    "               \"NRCCUA Name Buy\",\"NRCCUA RY2015\",\"PACAC\",\"PLAN Search List\",\"Primary Source\",\n",
    "               \"PSAT Search\",\"PTK\",\"SAT Score Data File\",\"Search Non-Responders for Jr-Soph RY2015\",\n",
    "               \"Spring Education Fair\",\"STATCO Inquiry Cards\",\"STATCO Inquiry cards - FR\",\n",
    "               \"STATCO Inquiry cards - GR\",\"STATCO Inquiry cards - TR\",\"Student Search Service\",\n",
    "               \"Summer Institute Data Load\",\"TOEFL Score Data File\",\"TOEFL Search Service\",\n",
    "               \"Venture Scholars\",\"Vietabroader\",\"WebAdmit Local Status CASPA\",\"WebAdmit Local Status PTCAS\",\n",
    "               \"WebAdmit Local Status SOPHAS\",\"WebAdmit CASPA Application\",\"WebAdmit GRE\",\n",
    "               \"WebAdmit Local Status\",\"WebAdmit MCAT\",\"WebAdmit PTCAS Application\",\n",
    "               \"WebAdmit SOPHAS Application\",\"WebAdmit TOEFL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interaction_list = [\"ACT Vendor Import\",\"ACTEOS Name Buy\",\"ACTEOS Senior Name Buy\",\"AP Name Buy\",\n",
    "                    \"AP Score Import\",\"Canadian Expo Affiliated Students\",\"Cappex\",\"Carnegie AC&U\",\n",
    "                    \"Carnegie College Xpress\",\"Carnegie PC&U\",\"CBSS Name Buy\",\"CBSS Senior Name Buy\",\n",
    "                    \"Chegg Domestic\",\"Chegg Global\",\"Chegg International\",\"Chegg Senior List\",\"CIS\",\n",
    "                    \"City Year\",\"College/School Inquiry\",\"Collegify Affiliated Students\",\n",
    "                    \"CommonApp InProgress\",\"Contact us Form\",\"Delete-do not use\",\"EdUSA\",\"Email Inquiry\",\n",
    "                    \"Generic Recruit Source\",\"GR Event\",\"GR GRE Name Buys\",\"Graduate CHSM Inquiries\",\n",
    "                    \"High School Scholars - Banner Recruit Migration\",\"IELTS Student Sent Test Score\",\n",
    "                    \"Inbound Email\",\"Inbound Phone\",\"Interview\",\"ISN\",\"MyMajors\",\"NHI Affiliated Students\",\n",
    "                    \"NRCCUA Name Buy\",\"NRCCUA Senior Name Buy\",\"Phone Call\",\"PLANEOS Name Buy\",\n",
    "                    \"Primary Source\",\"PSAT NameBuy\",\"PTK\",\"Returning to Learning-Affiliated Students\",\n",
    "                    \"SAT Name Buy\",\"SAT Senior Name Buy\",\"SAT Student Sent Test Scores\",\"Search PTK\",\n",
    "                    \"Search SNR-External Systems\",\"Student Call Center\",\"Summer Institute\",\"TOEFL Name Buy\",\n",
    "                    \"TOEFL Senior Name Buy\",\"TOEFL Student Sent Test Score\",\"TR Dual Admissions\",\"Travel\",\n",
    "                    \"Tuition Exchange\",\"UG Daily Visit\",\"UG Event\",\"UG Group Visit\",\n",
    "                    \"Unofficial Test Scores Entered\",\"Update List\",\"VietAbroader\",\"Walk-in\",\n",
    "                    \"WebAdmit CASPA Application\",\"WebAdmit GRE\",\"WebAdmit Local Status\",\"WebAdmit MCAT\",\n",
    "                    \"WebAdmit PTCAS Application\",\"WebAdmit SOPHAS Application\",\"WebAdmit TOEFL\",\n",
    "                    \"Zinch Affiliated Students\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = list(set(interaction_list) & set(source_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WebAdmit\\xa0SOPHAS Application',\n",
       " 'MyMajors',\n",
       " 'WebAdmit\\xa0TOEFL',\n",
       " 'Cappex',\n",
       " 'Carnegie PC&U',\n",
       " 'WebAdmit\\xa0CASPA Application',\n",
       " 'Carnegie AC&U',\n",
       " 'WebAdmit\\xa0GRE',\n",
       " 'PTK',\n",
       " 'GR GRE Name Buys',\n",
       " 'Carnegie College Xpress',\n",
       " 'Primary Source',\n",
       " 'High School Scholars - Banner Recruit Migration',\n",
       " 'WebAdmit\\xa0Local Status',\n",
       " 'NRCCUA Name Buy',\n",
       " 'WebAdmit\\xa0PTCAS Application',\n",
       " 'WebAdmit\\xa0MCAT']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
