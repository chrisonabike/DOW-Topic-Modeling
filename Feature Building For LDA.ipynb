{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "#nltk.download()  # Download text data sets, including stop words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_and_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)\n",
    "\n",
    "def add_double_title(text):\n",
    "    \"\"\"Add the title information to the beginning of the text (and double it, called 'weighting up')\"\"\"\n",
    "    try:\n",
    "        x = text['Title']\n",
    "        title = x + '. ' + x + '. '\n",
    "    except:\n",
    "        title = 'NO_TITLE'\n",
    "    try:\n",
    "        result = title + ' ' + text['Text']\n",
    "    except:\n",
    "        result = title\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def letters_only(text, field):\n",
    "    \"\"\"Replace all non-alphanumeric characters with a space\"\"\"\n",
    "    try:\n",
    "        x = re.sub(\"[^a-zA-Z]\",       # The pattern to search for\n",
    "                   \" \",               # The pattern to replace it with\n",
    "                   text[field] )      # The text to search\n",
    "    except:\n",
    "        return ('byte_code_error_ignore_this_ record')\n",
    "    return (x.lower())\n",
    "\n",
    "def remove_stop_words(text, field, stopwords_set):\n",
    "    \"\"\"Remove stop words from the review text\"\"\"\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Helper function to translate part of speech (POS) for us in the make_lemmas function.\n",
    "    nltk uses pos_tag to determine the POS that is not compatible with the wordnet_lemmatizer.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def make_lemmas(text, field):#, stopwords_set):\n",
    "    \"\"\"Toeknizes all words in the review and tags them with the part of speech (POS) they belong to\n",
    "    as a tuple. Each tuple (word, pos) is lemmatized before stop words are removed and the list is\n",
    "    joined back into a single item/doc\"\"\"\n",
    "    x = word_tokenize(text[field])\n",
    "    #x = word_tokenize(x)\n",
    "    x = nltk.pos_tag(x)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    doc = []\n",
    "    for word, part in x:\n",
    "        #doc.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "        doc.append(wnl.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "    #words = [w for w in doc if not w.lower() in stopwords_set]\n",
    "    #x = ( \" \".join( words ))\n",
    "    x = ( \" \".join( doc ))\n",
    "    return(x)\n",
    "\n",
    "def create_neg_stops():\n",
    "    \"\"\"Combine the original list of stop words the negative suffix. \n",
    "    For example:\n",
    "    'his', 'they', and 'me' become 'his_neg', 'they_neg', and 'me_neg'.\"\"\"\n",
    "    orig_stops = stopwords.words(\"english\")\n",
    "    neg_stops = []\n",
    "    for i in orig_stops:\n",
    "        neg_stops.append(i+'_neg')\n",
    "    orig_stops.extend(neg_stops)\n",
    "    return(orig_stops)\n",
    "\n",
    "def remove_negated_stop_words(text, field, neg_stops):\n",
    "    \"\"\"Remove all instances of stop words that have the '_neg' suffix.\"\"\"\n",
    "    # Make the text lowercase\n",
    "    x = text[field]\n",
    "    x = x.lower()\n",
    "    \n",
    "    stopwords_set = set(neg_stops)\n",
    "    \n",
    "    # List comprehension that splits the review into words and removes negative stop words\n",
    "    words = [w for w in text[field].split() if not w in stopwords_set]\n",
    "    return( \" \".join( words ))\n",
    "\n",
    "#/////////////////////ACCOUNT FOR NEGATION SENTIMENT///////////////////////////////////\n",
    "def negatize(text, field):\n",
    "    \"\"\"Use the NLTK library's mark_negation to find negative words (like 'not' and 'nor')\n",
    "    and append the '_neg' suffix to all words following the first negative word until it\n",
    "    encounters a period or comma.\n",
    "    Example: 'I don't like eating pizza, I love eating pizza' \n",
    "    Becomes: 'I don't like_neg eating_neg pizza_neg, I love pizza' \"\"\"\n",
    "    x = text[field]\n",
    "    \n",
    "    # The TextBlob class provides an easy way to split the reviews into sentences\n",
    "    x = TextBlob(x)\n",
    "    \n",
    "    piece = []\n",
    "    for sentence in x.sentences:\n",
    "        \n",
    "        # Split sentence on commas to ID phrases that need to be negated (if required)\n",
    "        part = re.split(', ',str(sentence))\n",
    "        for i in part:\n",
    "            piece.append(mark_negation(i.split()))\n",
    "    \n",
    "    # Combine all terms/phrases back to one doc\n",
    "    total = []\n",
    "    for terms in piece:\n",
    "        total.append(\" \".join(terms))\n",
    "    review = ''\n",
    "    for phrase in total:\n",
    "        review += phrase + ' '\n",
    "    \n",
    "    # mark_negation adds the _NEG suffix after the period, this catches those and fixes it\n",
    "    review = review.replace(\"._NEG\",\"_NEG.\")\n",
    "    review = review.lower()\n",
    "    \n",
    "    # return the entire entire review except for the last character which is always a space\n",
    "    return (review[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_scent(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    scents = ['scent','scents','scented','smell','smells','smelled','odor','odors',\n",
    "              'fragrance','fragrances','fragrant','aroma','aromas','perfume',\n",
    "              'perfumes','perfumed','whiff','fresh','freshness','stench','stink',\n",
    "              'stinks','smelly','pungent','stinky','odoriferous']\n",
    "    for word in x:\n",
    "        if word.lower() in scents:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_moisture(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    moisture = ['moisture','moist','wet','dry','damp','dampish','wettish', 'drenched',\n",
    "                'dripping','saturate','saturated','soaked','soaking','sodden','soggy',\n",
    "                'sopping','soppy','arid','dryness','dried','waterless','bone-dry',\n",
    "                'dehydrated','ultradry','dank','supple','quick-drying']\n",
    "    for word in x:\n",
    "        if word.lower() in moisture:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_residue(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    residue = ['streak','streaks','streaked','streaking','residue','film','cloudy',\n",
    "               'drop','drip','lines','drips','dripped','gunk','tarnish','spot',\n",
    "               'spots','drip-streaks']\n",
    "    for word in x:\n",
    "        if word.lower() in residue:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_value(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    value = ['value','price','prices','worth','worthless','cost','costs','expense',\n",
    "             'expensive','costly','overpriced','pricey','valuable','invaluable','cheap',\n",
    "             'economical','reasonable','inexpensive','expenses','valued','priced',\n",
    "             'cheapen','economics','low-priced','bargain','low-cost','budget',\n",
    "             'cheapest','bargains','money','budgeted','budgets']\n",
    "    for word in x:\n",
    "        if word.lower() in value:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_sensitivity(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    sensitivity = ['sensitivity','sensitive','skin','soft','softest','soften',\n",
    "                   'rough','rougher','smooth','touch']\n",
    "    for word in x:\n",
    "        if word.lower() in sensitivity:\n",
    "            return ('Yes')\n",
    "    return ('No')\n",
    "\n",
    "def tag_topics(text, field):\n",
    "    x = word_tokenize(text[field])\n",
    "    tags = {}\n",
    "    scent_tag = []\n",
    "    moisture_tag = []\n",
    "    residue_tag = []\n",
    "    value_tag = []\n",
    "    sensitivity_tag = []\n",
    "    scents = ['scent','scents','scented','smell','smells','smelled','odor','odors',\n",
    "              'fragrance','fragrances','fragrant','aroma','aromas','perfume','perfumes',\n",
    "              'perfumed','whiff','fresh','freshness','stench','stink','stinks','smelly',\n",
    "              'pungent','stinky','odoriferous']\n",
    "    \n",
    "    moisture = ['moisture','moist','wet','dry','damp','dampish','wettish', 'drenched',\n",
    "                'dripping','saturate','saturated','soaked','soaking','sodden','soggy',\n",
    "                'sopping','soppy','arid','dryness','dried','waterless','bone-dry','dehydrated',\n",
    "                'ultradry','dank','supple','quick-drying']   \n",
    "    \n",
    "    residue = ['streak','streaks','streaked','streaking','residue','film','cloudy','drop',\n",
    "               'drip','lines','drips','dripped','gunk','tarnish','spot','spots','drip-streaks']\n",
    "    \n",
    "    value = ['value','price','prices','worth','worthless','cost','costs','expense','expensive',\n",
    "             'costly','overpriced','pricey','valuable','invaluable','cheap','economical',\n",
    "             'reasonable','inexpensive','expenses','valued','priced','cheapen','economics',\n",
    "             'low-priced','bargain','low-cost', 'budget','cheapest','bargains','money',\n",
    "             'budgeted','budgets','discount']\n",
    "    \n",
    "    sensitivity = ['sensitivity','sensitive','skin','soft','softest','soften','rough',\n",
    "                   'smooth','touch']\n",
    "    \n",
    "    for word in x:\n",
    "        if word.lower() in scents:\n",
    "            scent_tag.append(word.lower())\n",
    "        if word.lower() in moisture:\n",
    "            moisture_tag.append(word.lower())\n",
    "        if word.lower() in residue:\n",
    "            residue_tag.append(word.lower())\n",
    "        if word.lower() in value:\n",
    "            value_tag.append(word.lower())\n",
    "        if word.lower() in sensitivity:\n",
    "            sensitivity_tag.append(word.lower())\n",
    "    if len(scent_tag) > 0:\n",
    "        tags['scent'] = scent_tag\n",
    "    if len(moisture_tag) > 0:\n",
    "        tags['moisture'] = moisture_tag\n",
    "    if len(residue_tag) > 0:\n",
    "        tags['residue'] = residue_tag\n",
    "    if len(value_tag) > 0:\n",
    "        tags['value'] = value_tag\n",
    "    if len(sensitivity_tag) > 0:\n",
    "        tags['sensitivity'] = sensitivity_tag\n",
    "        \n",
    "    if len(tags) == 0:\n",
    "        return ('')\n",
    "    return (tags)\n",
    "\n",
    "def get_topics(text):\n",
    "    topics = []\n",
    "    if (text['scent']) == 'Yes':\n",
    "        topics.append('scent')\n",
    "    if (text['moisture']) == 'Yes':\n",
    "        topics.append('moisture')\n",
    "    if (text['residue']) == 'Yes':\n",
    "        topics.append('residue')\n",
    "    if (text['value']) == 'Yes':\n",
    "        topics.append('value')\n",
    "    if (text['sensitivity']) == 'Yes':\n",
    "        topics.append('sensitivity')    \n",
    "    return( \", \".join( topics ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nouns_and_adjectives(text,field,stopwords_set):\n",
    "    \"\"\"Starts with the data that alread has had the stops removed. Tokenizes the text and uses nltk to\n",
    "    tag the parts of speech. If the word has a part of speech that is not a noun or adjective, it is not \n",
    "    included in the result.\"\"\"\n",
    "    x = word_tokenize(text[field])\n",
    "    #x = word_tokenize(x)\n",
    "    x = nltk.pos_tag(x)\n",
    "    doc = []\n",
    "    for word, part in x:\n",
    "        if part.startswith('J'):\n",
    "            # This is an adjective\n",
    "            doc.append(word)\n",
    "        if part.startswith('N'):\n",
    "            # This is a noun\n",
    "            doc.append(word)\n",
    "        #doc.append(wordnet_lemmatizer.lemmatize(word, pos=get_wordnet_pos(part)))\n",
    "    words = [w for w in doc if not w.lower() in stopwords_set]\n",
    "    x = ( \" \".join( words ))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data...\n",
      "Finished In:     0.913s.\n",
      "Adding titles to text...\n",
      "Finished In:     0.710s.\n",
      "Removing Non-Alphanumerics...\n",
      "Finished In:     2.403s.\n",
      "Removing Stop Words...\n",
      "Finished In:     1.808s.\n",
      "Lemmatizing The Reviews...\n",
      "Finished In:     399.572s.\n",
      "Lemmatizing The Sentences...\n",
      "Finished In:     514.582s.\n",
      "Use only Nouns and Adjectives...\n",
      "Finished In:     662.649s.\n",
      "Tagging topics for each review...\n",
      "Finished In:     35.059s.\n",
      "Tagging topics for each sentence...\n",
      "Finished In:     40.343s.\n",
      "FINISHED: \n",
      "Time Elapsed:    1659.530s.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "t1 = time()\n",
    "#/////////////////////READ THE DATA////////////////////////////////////////////////////\n",
    "print ('Reading the data...')\n",
    "wipes = pd.read_csv(\"home_products.csv\", header=0, encoding=\"ISO-8859-1\" )\n",
    "sentences = pd.read_csv(\"only_sentences.csv\", header=0, encoding=\"ISO-8859-1\" )\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////ADD THE TITLE TO THE TEXT////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Adding titles to text...')\n",
    "wipes['text_and_title'] = wipes.apply(lambda text: text_and_title(text), axis=1)\n",
    "wipes['double_title'] = wipes.apply(lambda text: add_double_title(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NON-ALPHANUMERICS AND PUNCTUATION/////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Non-Alphanumerics...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'text_and_title'), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: letters_only(text, 'double_title'), axis=1)\n",
    "sentences['lowercase_no_punctuation'] = sentences.apply(lambda text: letters_only(text, 'Sentence'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LOAD THE STOPWORDS PROVIDED BY NLTK//////////////////////////////\n",
    "stop1 = ['wipes', 'wipe', 'love', 'like', 'good', 'nice', 'amazon', 'loved', 'loves', 'likes', 'liked']\n",
    "stops = stopwords.words(\"english\")\n",
    "stops.extend(stop1)\n",
    "stopwords_set = set(stops)\n",
    "\n",
    "#/////////////////////REMOVE STOP WORDS///////////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Removing Stop Words...')\n",
    "wipes['text_and_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'text_and_title_no_stops', stopwords_set), axis=1)\n",
    "wipes['double_title_no_stops'] = wipes.apply(lambda text: remove_stop_words(text, 'double_title_no_stops', stopwords_set), axis=1)\n",
    "sentences['lowercase_no_stops'] = sentences.apply(lambda text: remove_stop_words(text, 'lowercase_no_punctuation', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////NEGATE TEXT AND TITLES///////////////////////////////////////////\n",
    "#t1 = time()\n",
    "#print ('Tagging Negative Text...')\n",
    "#wipes['text_and_title_negation'] = wipes.apply(lambda text: negatize(text, 'text_and_title'), axis=1)\n",
    "#wipes['double_title_negation'] = wipes.apply(lambda text: negatize(text, 'double_title'), axis=1)\n",
    "#print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////REMOVE NEGATIVE STOPS////////////////////////////////////////////\n",
    "#t1 = time()\n",
    "#print ('Removing Negative Stop Words...')\n",
    "#wipes['text_and_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'text_and_title_negation', create_neg_stops()), axis=1)\n",
    "#wipes['double_title_negation_no_stops'] = wipes.apply(lambda text: remove_negated_stop_words(text, 'double_title_negation', create_neg_stops()), axis=1)\n",
    "#print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////LEMMATIZE THE TEXT REVIEWS///////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Lemmatizing The Reviews...')\n",
    "wipes['lemma_text_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'text_and_title_no_stops'), axis=1)\n",
    "wipes['lemma_double_title_no_stops'] = wipes.apply(lambda text: make_lemmas(text, 'double_title_no_stops'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "t1 = time()\n",
    "print ('Lemmatizing The Sentences...')\n",
    "sentences['lemmatized'] = sentences.apply(lambda text: make_lemmas(text, 'lowercase_no_stops'), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////NOUNS AND AJECTIVES ONLY/////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Use only Nouns and Adjectives...')\n",
    "wipes['nouns_and_adjectives'] = wipes.apply(lambda text: nouns_and_adjectives(text, 'Text', stopwords_set), axis=1)\n",
    "sentences['nouns_and_adjectives'] = sentences.apply(lambda text: nouns_and_adjectives(text, 'Sentence', stopwords_set), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "#/////////////////////TAG TOPICS///////////////////////////////////////////////////////\n",
    "t1 = time()\n",
    "print ('Tagging topics for each review...')\n",
    "wipes['scent'] = wipes.apply(lambda text: tag_scent(text, 'Text'), axis=1)\n",
    "wipes['moisture'] = wipes.apply(lambda text: tag_moisture(text, 'Text'), axis=1)\n",
    "wipes['residue'] = wipes.apply(lambda text: tag_residue(text, 'Text'), axis=1)\n",
    "wipes['value'] = wipes.apply(lambda text: tag_value(text, 'Text'), axis=1)\n",
    "wipes['sensitivity'] = wipes.apply(lambda text: tag_sensitivity(text, 'Text'), axis=1)\n",
    "wipes['tags'] = wipes.apply(lambda text: tag_topics(text, 'Text'), axis=1)\n",
    "wipes['topics'] = wipes.apply(lambda text: get_topics(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "t1 = time()\n",
    "print ('Tagging topics for each sentence...')\n",
    "sentences['scent'] = sentences.apply(lambda text: tag_scent(text, 'Sentence'), axis=1)\n",
    "sentences['moisture'] = sentences.apply(lambda text: tag_moisture(text, 'Sentence'), axis=1)\n",
    "sentences['residue'] = sentences.apply(lambda text: tag_residue(text, 'Sentence'), axis=1)\n",
    "sentences['value'] = sentences.apply(lambda text: tag_value(text, 'Sentence'), axis=1)\n",
    "sentences['sensitivity'] = sentences.apply(lambda text: tag_sensitivity(text, 'Sentence'), axis=1)\n",
    "sentences['tags'] = sentences.apply(lambda text: tag_topics(text, 'Sentence'), axis=1)\n",
    "sentences['topics'] = sentences.apply(lambda text: get_topics(text), axis=1)\n",
    "print(\"Finished In:     %0.3fs.\" % (time()-t1))\n",
    "\n",
    "\n",
    "#/////////////////////WRITE THE DATA///////////////////////////////////////////////////\n",
    "wipes.to_csv('home_products_additional_features.csv', index=False)\n",
    "sentences.to_csv('sentences_additional_features.csv', index=False)\n",
    "\n",
    "print(\"FINISHED: \\nTime Elapsed:    %0.3fs.\" % (time() - t0))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
