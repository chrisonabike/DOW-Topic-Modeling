{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "reviews = pd.read_csv('home_products_additional_features.csv', header=0, encoding=\"ISO-8859-1\" )\n",
    "\n",
    "# Read in Reviews broken out by sentence\n",
    "sentences = pd.read_csv('sentence_home_products_additional_features.csv', header=0, encoding=\"ISO-8859-1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reviews.columns.values\n",
    "#reviews['Review Rating'][3]\n",
    "\n",
    "#reviews_20 = reviews[reviews['Review Rating']==20]\n",
    "#reviews_40 = reviews[reviews['Review Rating']==40]\n",
    "#reviews_60 = reviews[reviews['Review Rating']==60]\n",
    "#reviews_80 = reviews[reviews['Review Rating']==80]\n",
    "#reviews_100 = reviews[reviews['Review Rating']==100]\n",
    "\n",
    "#reviews[reviews['Review Rating']==100]\n",
    "#reviews['Review Rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_nmf(nmf_features, nmf_topics, nmf_top_words, nmf_data_samples, nmf_max_df, nmf_min_df, nmf_alpha, nmf_l1_ratio):\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=nmf_max_df, min_df=nmf_min_df,\n",
    "                                       max_features=nmf_features,\n",
    "                                       stop_words='english')\n",
    "\n",
    "    tfidf = tfidf_vectorizer.fit_transform(nmf_data_samples)\n",
    "\n",
    "    nmf = NMF(n_components=nmf_topics, random_state=1,\n",
    "              alpha=nmf_alpha, l1_ratio=nmf_l1_ratio).fit(tfidf)\n",
    "\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, nmf_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lda(lda_features, lda_topics, lda_top_words, lda_data_samples, lda_max_df, lda_min_df, lda_max_iter, lda_learning_offset):\n",
    "    print(\"Fitting LDA models with tf features...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=lda_max_df, min_df=lda_min_df,\n",
    "                                    max_features=lda_features,\n",
    "                                    stop_words='english')\n",
    "    \n",
    "    tf = tf_vectorizer.fit_transform(lda_data_samples)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_topics=lda_topics, max_iter=lda_max_iter,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=lda_learning_offset,\n",
    "                                    random_state=0)\n",
    "    \n",
    "    lda.fit(tf)\n",
    "    \n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, lda_top_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nulls(df, column):\n",
    "    data = df.dropna(subset = [column])\n",
    "    data = list(data[column])\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "clorox wipes family trust bleach thank know love household\n",
      "Topic #1:\n",
      "value pack money multi excellent containers useful super came\n",
      "Topic #2:\n",
      "easy makes use grab cleanup super wipes effective store\n",
      "Topic #3:\n",
      "smell fresh strong pleasant bleach harsh lemon makes chemicals\n",
      "Topic #4:\n",
      "stars described thank worked service excellent advertised ok didn\n",
      "Topic #5:\n",
      "love absolutely wipes especially em room having ease clorox\n",
      "Topic #6:\n",
      "good stuff quality pretty say size overall keyboard expensive\n",
      "Topic #7:\n",
      "clean way know fresh disinfected help ups feel surfaces\n",
      "Topic #8:\n",
      "best market thing tried disinfectant ve far brands cleaners\n",
      "Topic #9:\n",
      "bathroom sink toilet boys container especially kitchen room cleanings\n",
      "Topic #10:\n",
      "convenient super wipes powerful effective extremely way canister especially\n",
      "Topic #11:\n",
      "kitchen bath grease bathroom daily day surfaces room stove\n",
      "Topic #12:\n",
      "cleaning makes power easier surfaces powerful wiping breeze spills\n",
      "Topic #13:\n",
      "house room staple help live having rooms free germ\n",
      "Topic #14:\n",
      "disinfecting wipes surfaces power know lemon ease pack absolutely\n",
      "Topic #15:\n",
      "great item surfaces product service stuff hand cleanups dogs\n",
      "Topic #16:\n",
      "like feel don brands having dry chemicals new makes\n",
      "Topic #17:\n",
      "work desk fine wonders pretty just area phone say\n",
      "Topic #18:\n",
      "lysol wipes pack lime amazon say prefer containers usually\n",
      "Topic #19:\n",
      "wipe just grab away pull dry dust throw container\n",
      "Topic #20:\n",
      "really dust enjoy helps buying sure hands worth pick\n",
      "Topic #21:\n",
      "cleans leaves stuff way easily monitor excellent leaving residue\n",
      "Topic #22:\n",
      "quick ups cleanup cleanups way spills hand touch jobs\n",
      "Topic #23:\n",
      "buy bulk pack store money usually cheaper amazon save\n",
      "Topic #24:\n",
      "job does gets did says say dust doing fantastic\n",
      "Topic #25:\n",
      "messes little everyday ones tough quickly big easily toddler\n",
      "Topic #26:\n",
      "time long saver saves use save saving money family\n",
      "Topic #27:\n",
      "fast shipping dry delivery described cleanup super effective exactly\n",
      "Topic #28:\n",
      "awesome stuff super family think free gets regular thank\n",
      "Topic #29:\n",
      "used ve years dust far cleaned did silver new\n",
      "Topic #30:\n",
      "home classroom germ school free fresh help daily keeps\n",
      "Topic #31:\n",
      "disinfect way surfaces wipes quickly areas trust sick area\n",
      "Topic #32:\n",
      "works ok described doesn charm wonders advertised does mop\n",
      "Topic #33:\n",
      "scent lemon fresh leaves lime strong smelling lemony scented\n",
      "Topic #34:\n",
      "products bleach especially clorox toilet absolutely feel ve trust\n",
      "Topic #35:\n",
      "recommend highly definitely friends family recommended friend looking try\n",
      "Topic #36:\n",
      "better spray tried ve brands cloth paper towel feel\n",
      "Topic #37:\n",
      "handy come super having quickly comes little jobs spills\n",
      "Topic #38:\n",
      "disinfects deodorizes makes whitens surfaces powerful enjoy kills fact\n",
      "Topic #39:\n",
      "amazing absolutely literally stuff thing thank simply try children\n",
      "Topic #40:\n",
      "counter tops door knobs table wiping toilet handles stove\n",
      "Topic #41:\n",
      "bathrooms kitchens kitchen sink daily bottle having toilet canister\n",
      "Topic #42:\n",
      "leave don streaks residue smelling glass dry leaves doesn\n",
      "Topic #43:\n",
      "make easier life sure breeze feel makes just look\n",
      "Topic #44:\n",
      "brand trust trusted store compare new tried brands prefer\n",
      "Topic #45:\n",
      "use daily everyday day basis wipes easy countertops continue\n",
      "Topic #46:\n",
      "counters wiping spills door sinks knobs toilet stove handles\n",
      "Topic #47:\n",
      "germs kill know killing kills away germ free family\n",
      "Topic #48:\n",
      "need quickly spray paper grab don hand towels hurry\n",
      "Topic #49:\n",
      "things disinfected free germ keeps different little makes know\n",
      "Topic #50:\n",
      "price amazon reasonable beat staples right pack quality shipping\n",
      "Topic #51:\n",
      "using years enjoy feel started ve makes wipes ease\n",
      "Topic #52:\n",
      "mess throw big away way grab sticky quickly making\n",
      "Topic #53:\n",
      "smells fresh smelling kills disinfectant looks know bleach bad\n",
      "Topic #54:\n",
      "stainless steel appliances new weiman fingerprints fridge look shine\n",
      "Topic #55:\n",
      "perfect size day wiping lens windows bath busy table\n",
      "Topic #56:\n",
      "glasses excellent lens lenses eye screens phone camera clear\n",
      "Topic #57:\n",
      "nice scent hands soft size moist shine large pretty\n",
      "Topic #58:\n",
      "expected exactly worked did came received normal delivered got\n",
      "Topic #59:\n",
      "car purse interior inside leather carry travel seats room\n",
      "Topic #60:\n",
      "product excellent great family thanks fantastic thank happy advertised\n",
      "Topic #61:\n",
      "cleaner floor floors spray glass toilet tile water hoover\n",
      "Topic #62:\n",
      "old year son month messy loves especially boy running\n",
      "Topic #63:\n",
      "wonderful scent thanks spills everyday care makes silver day\n",
      "Topic #64:\n",
      "convenience ease enjoy power beat quality ability having cleanups\n",
      "Topic #65:\n",
      "small children especially big young jobs pets spills size\n",
      "Topic #66:\n",
      "kids help having school toys sick messy dogs especially\n",
      "Topic #67:\n",
      "wipes container dry disinfectant moist fantastic say hand wet\n",
      "Topic #68:\n",
      "favorite far absolute hands new thing cleaners supply subscribe\n",
      "Topic #69:\n",
      "deal save subscribe got amazon pack coupon buying money\n",
      "Topic #70:\n",
      "office desk people space area phone school overpowering equipment\n",
      "Topic #71:\n",
      "screen computer screens laptop phone staples monitor tv streaks\n",
      "Topic #72:\n",
      "lot easier help silver bought life big surfaces think\n",
      "Topic #73:\n",
      "season flu cold especially door desks sick knobs handles\n",
      "Topic #74:\n",
      "no_title thank super surface absolutely hand everyday durable affordable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#////////////////////////SET NMF PARAMETERS AND RUN MODEL/////////////////////////////\n",
    "\n",
    "# Uncomment one of the lines (and only one line) that begins with nmf_data_samples to change the data set the model runs on\n",
    "#nmf_data_samples = remove_nulls(reviews, 'Text')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'Title')\n",
    "nmf_data_samples = remove_nulls(reviews, 'text_and_title')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'double_title')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'text_and_title_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'double_title_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'text_and_title_negation')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'double_title_negation')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'text_and_title_negation_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'double_title_negation_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'lemma_text_title_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'lemma_double_title_no_stops')\n",
    "#nmf_data_samples = remove_nulls(reviews, 'nouns_and_adjectives')\n",
    "\n",
    "nmf_features = 12000      # Size of the vocabulary\n",
    "nmf_topics = 75           # Number of topics\n",
    "nmf_top_words = 9         # Words to include in the topic\n",
    "nmf_max_df=0.95           # Ignore terms that have a doc frequency (percent or int) strictly higher than the given threshold\n",
    "nmf_min_df=2              # Ignore terms that have a doc frequency (percent or int) strictly lower than the given threshold\n",
    "nmf_alpha=.1              # Constant that multiplies the regularization terms. Set to zero for no regularization.\n",
    "nmf_l1_ratio=.5           # Regularization mixing parameter.  0 <= l1_ratio <= 1\n",
    "\n",
    "run_nmf(nmf_features, nmf_topics, nmf_top_words, nmf_data_samples, nmf_max_df, nmf_min_df, nmf_alpha, nmf_l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "great clorox wipes price excellent switches beat bonus true\n",
      "Topic #1:\n",
      "counters office phone desk disinfecting purse uses desks cat\n",
      "Topic #2:\n",
      "lots sturdy trick ton convienent described bath baths general\n",
      "Topic #3:\n",
      "order months live packs green cheap damage hot task\n",
      "Topic #4:\n",
      "value dogs continue feet dish play linoleum sense ball\n",
      "Topic #5:\n",
      "room need easier size children better wipes bleach spots\n",
      "Topic #6:\n",
      "delivery pair options mother chemicals close trust costco cat\n",
      "Topic #7:\n",
      "favorite wipes wish liquid end reason disappointed roll eyeglass\n",
      "Topic #8:\n",
      "handy stainless steel wipes stains look scrub marks pets\n",
      "Topic #9:\n",
      "bathroom water everyday try keeps case soap seat past\n",
      "Topic #10:\n",
      "review grandson helpful reading tube packs ability reasons thank\n",
      "Topic #11:\n",
      "right recommend regular wipes action dual contact routine sides\n",
      "Topic #12:\n",
      "wife complaint rip bargain biggest unable feed waste process\n",
      "Topic #13:\n",
      "kitchen lysol wipes mess counter tv odor seconds toddlers\n",
      "Topic #14:\n",
      "ones honest kill similar peace college trash garage short\n",
      "Topic #15:\n",
      "glasses dirty convenience long help classroom white eye wood\n",
      "Topic #16:\n",
      "little works moist husband skin carry disposable gloves star\n",
      "Topic #17:\n",
      "easy simple travel healthy spill list hurry access tube\n",
      "Topic #18:\n",
      "wonderful huge ok difference come sanitary spotless pay micro\n",
      "Topic #19:\n",
      "pack big awesome wipes fact bag durable jobs mop\n",
      "Topic #20:\n",
      "bacteria reviews read gross common virus saw advertised television\n",
      "Topic #21:\n",
      "thanks buff coat vehicles warm boys rinse wrong streaks\n",
      "Topic #22:\n",
      "wipes canister containers entire place confident pre stay market\n",
      "Topic #23:\n",
      "germs tops school sink tables mirrors course nasty worry\n",
      "Topic #24:\n",
      "dry paper towels towel cloths dries count effort space\n",
      "Topic #25:\n",
      "items buy alcohol single stores idea stock special smart\n",
      "Topic #26:\n",
      "affordable kinds lol reorder mud buff know list count\n",
      "Topic #27:\n",
      "grease baby pleasant grime spot wipes cut remove essential\n",
      "Topic #28:\n",
      "car dust residue wipes leaves wash rid staples doors\n",
      "Topic #29:\n",
      "camera today awhile compare balls road review choice formula\n",
      "Topic #30:\n",
      "products perfect item extra air packaging eyeglasses hold grout\n",
      "Topic #31:\n",
      "free cleans bathrooms different toilet safe germ dog phones\n",
      "Topic #32:\n",
      "convenient super disinfects life week puppy natural ready rag\n",
      "Topic #33:\n",
      "cleaner box old thing year pull tile oil version\n",
      "Topic #34:\n",
      "small floor hard boys handle hoover rags reach horrible\n",
      "Topic #35:\n",
      "laptop smudges son glad shipping iphone watch grimy customer\n",
      "Topic #36:\n",
      "best wipes deal stuff mom lid ease boxes toddler\n",
      "Topic #37:\n",
      "way areas chemicals fast power amazing thicker various bottles\n",
      "Topic #38:\n",
      "large cleaners shiny multi addition walk icky supply game\n",
      "Topic #39:\n",
      "smell quick strong wipes ups people stars cleanup chemical\n",
      "Topic #40:\n",
      "daily silver needs polish basis wipes terrible bought experience\n",
      "Topic #41:\n",
      "product buying toxic freak okay non cons absolute ways\n",
      "Topic #42:\n",
      "monitor sheets lint individual start environment poor outside instructions\n",
      "Topic #43:\n",
      "new fine wipes fingerprints shine dispenser tear issues monitors\n",
      "Topic #44:\n",
      "messy recommended disinfection parents mold manner game sickness great\n",
      "Topic #45:\n",
      "bit food refrigerator finger prints purpose raw sponge particles\n",
      "Topic #46:\n",
      "kind mind think texture lime fragrance pricey blossom prices\n",
      "Topic #47:\n",
      "wet quality wipes problem clear overall finish half opinion\n",
      "Topic #48:\n",
      "hand subscribe cheaper cost discount wipes local option teacher\n",
      "Topic #49:\n",
      "smaller material packets packages complaints quantity saves book towel\n",
      "Topic #50:\n",
      "thank greasy swipe fit toss loves chance boy gunk\n",
      "Topic #51:\n",
      "container times wipes care difficult second weeks sheet guess\n",
      "Topic #52:\n",
      "lens bulk service plan customer usage parents coffee traditional\n",
      "Topic #53:\n",
      "spray streaks wipes appliances lenses bottle streak countertops zeiss\n",
      "Topic #54:\n",
      "purchase thought supplies pleased future computers pledge beats performance\n",
      "Topic #55:\n",
      "years wipes possible face number change vehicle nose attention\n",
      "Topic #56:\n",
      "home package happy plastic furniture machine child pieces canisters\n",
      "Topic #57:\n",
      "clean wipes know toys month tablet dont sanitize gentle\n",
      "Topic #58:\n",
      "kids floors feel table sinks walls toilets hardwood chairs\n",
      "Topic #59:\n",
      "things store moisture high daughter grocery chair club soapy\n",
      "Topic #60:\n",
      "house messes spills wipes rooms night fingers minute line\n",
      "Topic #61:\n",
      "cloth area soft microfiber stove pet efficient vinegar dryer\n",
      "Topic #62:\n",
      "surface glass grab touch tried breeze windex stick heavy\n",
      "Topic #63:\n",
      "work brand brands disinfectant solution trust results company black\n",
      "Topic #64:\n",
      "time cleaning wipes dirt money sticky able bad worth\n",
      "Topic #65:\n",
      "scent surfaces disinfect fresh lemon light scents fan want\n",
      "Topic #66:\n",
      "sick real issue places mirror paws previous streaky business\n",
      "Topic #67:\n",
      "enjoy looks period protection chance cheap play powder dusty\n",
      "Topic #68:\n",
      "use wipes day hands door effective open larger fantastic\n",
      "Topic #69:\n",
      "left mud bags gotten soda wide look bike diaper\n",
      "Topic #70:\n",
      "job computer screens smells wipes leave smears decent used\n",
      "Topic #71:\n",
      "screen lot wipes couple film hair available minutes days\n",
      "Topic #72:\n",
      "family sure household expensive wipes waste friends sale staple\n",
      "Topic #73:\n",
      "electronics duster golf clubs admit ball freshen dusting balls\n",
      "Topic #74:\n",
      "flu season cold useful kills viruses snap restroom men\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#////////////////////////SET LDA PARAMETERS AND RUN MODEL/////////////////////////////\n",
    "\n",
    "# Uncomment one of the lines (and only one line) that begins with lda_data_samples to change the data set the model runs on\n",
    "\n",
    "#lda_data_samples = remove_nulls(reviews, 'Text')\n",
    "#lda_data_samples = remove_nulls(reviews, 'Title')\n",
    "#lda_data_samples = remove_nulls(reviews, 'text_and_title')\n",
    "#lda_data_samples = remove_nulls(reviews, 'double_title')\n",
    "#lda_data_samples = remove_nulls(reviews, 'text_and_title_no_stops')\n",
    "#lda_data_samples = remove_nulls(reviews, 'double_title_no_stops')\n",
    "#lda_data_samples = remove_nulls(reviews, 'text_and_title_negation')\n",
    "#lda_data_samples = remove_nulls(reviews, 'double_title_negation')\n",
    "#lda_data_samples = remove_nulls(reviews, 'text_and_title_negation_no_stops')\n",
    "#lda_data_samples = remove_nulls(reviews, 'double_title_negation_no_stops')\n",
    "#lda_data_samples = remove_nulls(reviews, 'lemma_text_title_no_stops')\n",
    "\n",
    "#lda_data_samples = remove_nulls(reviews, 'lemma_double_title_no_stops')\n",
    "#lda_data_samples = remove_nulls(reviews_20, 'lemma_double_title_no_stops')\n",
    "\n",
    "#lda_data_samples = remove_nulls(reviews, 'nouns_and_adjectives')\n",
    "#lda_data_samples = remove_nulls(reviews_100, 'nouns_and_adjectives')\n",
    "\n",
    "\n",
    "#lda_data_samples = remove_nulls(sentences, 'Sentence')\n",
    "#lda_data_samples = remove_nulls(sentences, 'sentence_no_stops')\n",
    "#lda_data_samples = remove_nulls(sentences, 'sentence_lemma_no_stops')\n",
    "lda_data_samples = remove_nulls(sentences, 'sentence_nouns_and_adjectives')\n",
    "\n",
    "\n",
    "lda_features = 12000      # Size of the vocabulary \n",
    "lda_topics = 75           # Number of topics\n",
    "lda_top_words = 9         # Words to include in the topic\n",
    "lda_max_df= 0.80          # Ignore terms that have a doc frequency (percent or int) strictly higher than the given threshold\n",
    "lda_min_df= 15             # Ignore terms that have a doc frequency (percent or int) strictly lower than the given threshold\n",
    "lda_max_iter=6            # Number of iterations to compute\n",
    "lda_learning_offset=40.   # A parameter that downweights early iterations in online learning. Should be > 1\n",
    "\n",
    "run_lda(lda_features, lda_topics, lda_top_words, lda_data_samples, lda_max_df, lda_min_df, lda_max_iter, lda_learning_offset)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
